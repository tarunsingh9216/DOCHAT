{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Bcm05TTJ4d3",
    "outputId": "a616bccc-6f83-4f74-c876-ec50462c6938"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.0/76.0 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PtiwsqwwKOEO"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    HfArgumentParser,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SkTp4ceVKOg_",
    "outputId": "95e75210-027d-4b0c-d0b8-c48856a65e3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"GPU Available:\", torch.cuda.is_available()) # checking that the GPU is availabe or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QJPc2DKKOwu"
   },
   "outputs": [],
   "source": [
    "# Define the Model name, and Data set\n",
    "model_name = \"NousResearch/Llama-2-7b-chat-hf\"\n",
    "dataset_name = \"mlabonne/guanaco-llama2-1k\"\n",
    "new_model = \"Llama-2-7b-Dochat-finetune\"\n",
    "\n",
    "# Define the PEFT(QLoRA) Parameters\n",
    "QLora_parm = {\n",
    "    \"lora_r\" : 64,\n",
    "    \"lora_alpha\" :16,\n",
    "    \"lora_dropout\" : 0.1\n",
    "}\n",
    "\n",
    "# Define Quantize Parameters\n",
    "BitsBytes_parm = {\n",
    "    \"use_4bit\": True,\n",
    "    \"bnb_4bit_compute_dtype\" : \"float16\",\n",
    "    \"bnb_4bit_quant_type\" : \"nf4\",\n",
    "    \"use_nested_quant\" : False\n",
    "}\n",
    "\n",
    "# Define the Traning Parameters\n",
    "training_parm = {\n",
    "    \"per_device_train_batch_size\" : 1,\n",
    "    \"gradient_accumulation_steps\" :2,\n",
    "    \"warmup_steps\" : 2,\n",
    "    \"max_steps\" : -1,\n",
    "    \"learning_rate\" :2e-4,\n",
    "    \"num_train_epochs\" : 1,\n",
    "    \"fp16\" : False,\n",
    "    \"logging_steps\" :25,\n",
    "    \"output_dir\" : \"./results\",\n",
    "    \"remove_unused_columns\" :False,\n",
    "    \"report_to\" : \"none\",\n",
    "    \"optim\" : \"paged_adamw_32bit\"\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621,
     "referenced_widgets": [
      "431d4618b2d745c1a726dcef330c1302",
      "37bbf1c839834a62b0703174c970171b",
      "1a0204a007a44939b3c6bfa09583ee02",
      "1d96c97086824cd7839e7d9634bd0fa1",
      "96110f811ae743898df83e725375f1c6",
      "e5cbe543d0024abcb446a9218a67835f",
      "d03087ada2c14385a12afb27455dac41",
      "44bc593bdc3f43e98b4eb79d68b0917b",
      "9fd40743d1444fe59dfffd0008020ad6",
      "aef99cfdbeb94e06a34eb6382239dd2c",
      "04040fab5a6f471f9376b94305cd80fa",
      "a34125b860b54d20ac7e3207f3040a77",
      "a3beaffd1592495eb8a33f77c7862c43",
      "6e068dd9a7c84fee8443013a2fabc014",
      "179a3baaf8c14a69b519e9f888172d3e",
      "20c915e17b7a461ab2718430624c262f",
      "301d88060f0b4d0fb0a14357d6b67fdd",
      "c367ab7ee6ad46a48a2e23e86e77a5ac",
      "a52ffca46ef94c809d784b3a6449ba74",
      "9ec1f8aa1c704c9fab0306496d4063b1",
      "8eeeee7f715c40808f83e12b1dd96ecd",
      "b9e2b59858e048f0a6c3eeb810a8b1a5",
      "ebb878851e0d440f91dad53af29adade",
      "01e12d2bbc3b40688ac054a995ee6b0a",
      "899d226a4ed8440a83b966e225f3521b",
      "cd49dda38e5a44159863396a390f63c9",
      "a614c0b91fe44bd8af57761c3565c932",
      "109b4cf2d1c84ae89b40593a58a154a2",
      "d22289f4353942d8ab774c35dd996b1a",
      "daebecebcb7045c59244c65bce52e7fa",
      "6aaf85863c5047bdb69124f8b8699474",
      "1f43273ae11242ad8ef2ad180c64f26d",
      "b2b148e7a8bd4e5c86ee1d056720a94d",
      "e544bc72607e4d7599699b8e95bb5799",
      "c94675cae76049ceb49a61789faa3950",
      "f39a1dd8e06148bcae513011d33c598d",
      "38ed9381dc5e44c3b1c4bb60e85606ba",
      "d9f98eb02af649a3b6dfa41b04e2c11b",
      "90afb6dff70e484c84331c600b487a47",
      "ddf266c306064c41a800b2515f552aac",
      "02ec746c5db8493685d2e10567386b22",
      "865d24cf2cb34116b553046ef23d2890",
      "5e64e12a5da04981bbd09d429fc6e92e",
      "58006d5099024ad6b4153871b53738a1",
      "de6b5b389517467285b86ae999c1bd93",
      "e59dd185aeb9499982725081f5ed6b95",
      "b90388e241c74a4f8b027b1ade67aaed",
      "71d1fe20938846e7afc811a70155ee2b",
      "523838e6a6ec468ca0fae20117bd667c",
      "3812fcaec23c43c69fd8baf420c7b6a1",
      "ec9904ce96b54abc8ef454db58f761bb",
      "2f0dcc1b4b0545c892650631691fc939",
      "606f48b1367d4982beb3ea062b82de4e",
      "86290da2d1814d8ea6aa0c16034915b2",
      "639b0089214b44879e17c7fc2d8de66b",
      "01533bbe865440ab9260507839d090f2",
      "e3927728e84d4b49afd0c9962504e46b",
      "ee8f8b3fc942450fb9ce9c50da140be7",
      "937d12ed6a684724886be358eaec9e3f",
      "a08da3c7c36b433eafa8eee5f8690d4f",
      "76e7cfc80d1d493aaf66f3b4a884d17b",
      "e21cd3794b88402dbc9442bc51491147",
      "914867846d8041ca89adbb8e4dc87f66",
      "348217ffe6d64fe2bbca53e896b7099f",
      "cef4df0c13814058b4c0d4d91df8664d",
      "8baef3b12d404e36a5cebad6f062b761",
      "0bb65242f5814fb3b6f701d20c0257e0",
      "efe50df0de514d468001ec9618d8f2c1",
      "e5be28a5850e4fd8b0ba86363d61f7d2",
      "212fded68c4849f28e1973136b2df7a3",
      "a36d2765c1114dd39ceb95107ee373b5",
      "6e3a4961532e44639f70654b665c4aa7",
      "0cc778cf61f04afa9b43e11b208fe3ba",
      "1c731c0efecd4141b80fe5713285955b",
      "6763cb7aa01a427ea8f8ffbc8878945d",
      "cbaaeb63b6f7428ebe1c5e967aabb2bc",
      "1c680ab0097241b6990b0f00edeca736",
      "f692fc45a1414e708385a867301823ec",
      "6dcff7733b874927841ca3f32f5609d5",
      "06403146ce9445d3998fe9a885abcffa",
      "8b1f7939dd1247639ed482a20a3d228b",
      "9b322cebacfb4eb98f4d3e519348992d",
      "dc4e4bf4693545439cca5beba621de8f",
      "e3d449fc8699444ba1e72c170ac2f7f7",
      "22117eaa3ba8457095474e6b8ba9be11",
      "3dba6da445ef4643b616375f16018555",
      "c2b463d8ace54da7808dfbf9fb539060",
      "4f943f0432134afb86b79c0150516f78",
      "130566fb14894ebcbc85c34d55a8865c",
      "01c908349ad744f8ae17588c2ae78584",
      "0d9eb5a764bc4742ad8c89dda56dab57",
      "9135f8223cde43a5b5690f2b4c3d0996",
      "6ae76631c9f64f2a876ad53a2db80e93",
      "edbe95fa4e2e411c97c0a64174cc4aec",
      "d5faf131585f435eb76886f42e56031a",
      "a1bba14084474ad7bb5f02fd7e3bda4c",
      "ec9532b43fac4beeaa4b0efb52e38fb9",
      "7652f486a858421b8d3db713217ed062",
      "b69ab20b558f4c70bef5195a29e4cc69",
      "6a58c5c947934e70a1145ce4feaf65b5",
      "378ea4a92c104a75b741125f7fa697c7",
      "4c0bab7f41d34bbba74e5d29006c8893",
      "688b1c8091074abfa72accf19f4cc6ae",
      "fcd5f56e73484383ae42508bbf32feec",
      "d9eb6112a15642b196385d3e844c8987",
      "068a9600b40a4bcda46c8911ea6774f0",
      "cbc8769ca10a4ed6ad900201df3e6b8e",
      "5bc53e85a4364798809e61094886c5a1",
      "1087292549f1488bae1764ed1e7e8df1",
      "d1bdfb3052a349eab77b02d978ba50c7",
      "1ef01472480b4576bcf5f8abb2ae72b4",
      "a37aabb873694d879fbe16ed1fc90174",
      "559aaf0a0fb74b4fb14a9d4bc43607e5",
      "6bfa7ee189934206a1050f70cb29be28",
      "21b4eec0d8d342bb87390663988c08d8",
      "4b6338c56f4b4d4599dd343522b4528e",
      "8903dd65cd344db7a837b46d47ff245d",
      "31a87627e0e34c1fbdefea4eebdc2e55",
      "2caf322309654222a35dd930170c82bb",
      "1c98f2f0b7a041eca598f570e673c007",
      "70a91c31602e4dd5b4d7358bf8bf75fb",
      "3c668b7304a44fb9a4f75810a7a8018d",
      "749c22a01bce41378b6e6e60b127a2ba",
      "f77f949e8ce14d428e7bbc03d1520fe1",
      "404407e62b2542eaa7476636411fe053",
      "44ef92730fba40978fc4683718a6b74d",
      "de6c05a78d7449c891366c6d469e4516",
      "1c83f81bd4dd4c15a489429558198ada",
      "943ee4222cbd48c88efb1e5eb91b227b",
      "c290f55934094e11b5442b1bff9db7fc",
      "c3b26eec98714dd78bfc8cf2eade3f8c",
      "1b8a4ee176974503a838a1ff02e2466d",
      "88777b8769804d71bb1fd2048862ae36",
      "09bdc39234654585bfa57cbd34c44269",
      "2566451f9e7e4b188b2d8caca183c206",
      "8fac6fab67424355993e591fcbe8ae42",
      "351fbc013429468c9937e898b475ddf6",
      "a28bc040e36b4579a4d05360ab5a01e6",
      "d5fa9687672740bc89b55ae078f3a4e5",
      "c58ae75768004eaaafbc4804d99cb3d1",
      "590a52af38074984b93200e9a4bef8a3",
      "4d744726e8954485aa6a6ce4c05b07c6",
      "c36dbf76a5164d97b54d645ffb5212a3",
      "ca839fe8d46c49a3b0fc0a970fc5a4c1",
      "4081cc1b0b804e5893221a6f70abdf15",
      "975da6540e03476a81586635978a27ac",
      "b1f54325005546d485a6808c65ca622c",
      "b4b77c4f4f4446e6864b6f117be1aed4",
      "cff9d1b3609743d690305f92ef1eb880",
      "c62a1ab5f22a4c50bba160e96bec090f",
      "36a7d006e6d840bf847621cead043d95",
      "88f770f445b849e197f40b68a2381f45",
      "782d779c917641ae950c95c1de2afc6b",
      "3447062ae1db45b0891c064ee71a7b57",
      "0f575b7b4187449aaf51197561a0ffcb",
      "b77aaca56a0c475fa73800e7443ded33",
      "b2aa87f1d3bc47b9b02d3ce1ddb3582c",
      "eea4c478160046269f70e65f6ef390b0",
      "733be5aea23c4169a29633101fb4b9d1",
      "6f893ca5d3ce435797f560cd3553ad6c",
      "3d7a07f2644b442eab1fe20563a26884",
      "c8d9071710064a719389d5ad677733a0",
      "43f098fca0b94924901ef371866b63ce",
      "6e9bf748c0ab4614ac13ca44c40fc00b",
      "be1276b7a95d4eb8b23db64544707bf8"
     ]
    },
    "id": "oS9jUoDTKO8h",
    "outputId": "4d372b79-ffed-4b89-a16f-fdce236d7e28"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431d4618b2d745c1a726dcef330c1302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34125b860b54d20ac7e3207f3040a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-9ad84bb9cf65a42f.parquet:   0%|          | 0.00/967k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebb878851e0d440f91dad53af29adade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e544bc72607e4d7599699b8e95bb5799",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6b5b389517467285b86ae999c1bd93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01533bbe865440ab9260507839d090f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb65242f5814fb3b6f701d20c0257e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f692fc45a1414e708385a867301823ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "130566fb14894ebcbc85c34d55a8865c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a58c5c947934e70a1145ce4feaf65b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ef01472480b4576bcf5f8abb2ae72b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c668b7304a44fb9a4f75810a7a8018d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88777b8769804d71bb1fd2048862ae36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca839fe8d46c49a3b0fc0a970fc5a4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f575b7b4187449aaf51197561a0ffcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(dataset_name, split='train')\n",
    "\n",
    "compute_dtype = getattr(torch, BitsBytes_parm['bnb_4bit_compute_dtype'])\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "  load_in_4bit = BitsBytes_parm['use_4bit'],\n",
    "  bnb_4bit_quant_type = BitsBytes_parm['bnb_4bit_quant_type'] ,\n",
    "  bnb_4bit_compute_dtype = compute_dtype,\n",
    "  bnb_4bit_use_double_quant = BitsBytes_parm['use_nested_quant']\n",
    ")\n",
    "\n",
    "# check GPU compatibility with bfloat16\n",
    "if compute_dtype == torch.float16 and BitsBytes_parm['use_4bit']:\n",
    "  major, _ = torch.cuda.get_device_capability()\n",
    "  if major >= 8:\n",
    "    print(\"=\", * 80)\n",
    "    print(\"Your GPU supports bfloat16: accelerate traning with bf16=True\")\n",
    "    print(\"=\", * 80)\n",
    "\n",
    "\n",
    "# Load Base Model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  quantization_config = bnb_config,\n",
    "  device_map = {\"\": 0}\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load LLaMA Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "\n",
    "# Load LoRA Config\n",
    "peft_config = LoraConfig(\n",
    "  lora_alpha = QLora_parm['lora_alpha'],\n",
    "  lora_dropout = QLora_parm['lora_dropout'],\n",
    "  r = QLora_parm['lora_r'],\n",
    "  bias = 'none',\n",
    "  task_type = \"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "\n",
    "# Load Traning Arguments\n",
    "training_arg = TrainingArguments(\n",
    "    per_device_train_batch_size = training_parm['per_device_train_batch_size'],\n",
    "    gradient_accumulation_steps= training_parm['gradient_accumulation_steps'],\n",
    "    warmup_steps= training_parm['warmup_steps'],\n",
    "    max_steps= training_parm['max_steps'],\n",
    "    learning_rate= training_parm['learning_rate'],\n",
    "    num_train_epochs= training_parm['num_train_epochs'],\n",
    "    fp16= training_parm['fp16'],\n",
    "    logging_steps= training_parm['logging_steps'],\n",
    "    output_dir= training_parm['output_dir'],\n",
    "    remove_unused_columns= training_parm['remove_unused_columns'],\n",
    "    report_to = training_parm['report_to'],\n",
    "    optim= training_parm['optim']\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q7FZ3ornKPE6"
   },
   "outputs": [],
   "source": [
    "# This function will print the NO. of Trainable Parameters in the Model\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yEcjvKFXKPMT",
    "outputId": "81db601f-62bf-4bb2-c7c1-c10abefefbf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 262410240 || all params: 3500412928 || trainable%: 7.496550989769399\n",
      "trainable params: 33554432 || all params: 3533967360 || trainable%: 0.9494833591219133\n"
     ]
    }
   ],
   "source": [
    "print_trainable_parameters(model) # No. of trainable parameters before adding the PEFT (fine tunning)\n",
    "model = get_peft_model(model, peft_config) # apply the fine-tunning\n",
    "print_trainable_parameters(model) # no. of trainable parameters after adding the PEFT (fine tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "31a4c3ae196d4deab51f1e6fb88fc3db",
      "d4c10cb5e72e4a58a675336af39a729a",
      "0f9facb4b2794cedb71a18fbda726ece",
      "427fd264784741ecb5d223d4a1d0fdc3",
      "7e24fb86e03d4e4780d51fef4f276d9b",
      "21b4732ecf7e4c7e89629233e3b83600",
      "2089b4fedfc14305af735bfca96763f0",
      "068c08411a3144f7b67370bdc01de90e",
      "60b0eac9b12f460e9598cda8ce27dad9",
      "07246f15e9e74e698261249292a510b3",
      "1abc179c119e4ac89abe94c57652c089"
     ]
    },
    "id": "5SDTd9rTKPTt",
    "outputId": "9a10a885-9dce-4fa3-f245-83a19e0e57b4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31a4c3ae196d4deab51f1e6fb88fc3db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenizer (tokenize the data set)\n",
    "def tokenize_function(example):\n",
    "        return tokenizer(\n",
    "          example[\"text\"],\n",
    "          padding=\"max_length\",\n",
    "          truncation=True,\n",
    "          max_length=512,\n",
    "          return_tensors=\"pt\"\n",
    "        )\n",
    "data = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "id": "ZOWM-ZQKKPbc",
    "outputId": "72b1ccf4-09b2-457f-bab1-ae4667e50799"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 19:12, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.591200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.366500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>1.290300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>1.362300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.315400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>1.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.308800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>1.291700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.181000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>1.274400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.299800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>1.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.174600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.375400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.298200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>425</td>\n",
       "      <td>1.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.280900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>475</td>\n",
       "      <td>1.216600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.234200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=1.3066954307556153, metrics={'train_runtime': 1156.5668, 'train_samples_per_second': 0.865, 'train_steps_per_second': 0.432, 'total_flos': 2.0400838803456e+16, 'train_loss': 1.3066954307556153, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the Model on customize Data set\n",
    "# Use the Trainer (from transformers) to train the Model you can also use the SFTTrainer.\n",
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer = tokenizer,\n",
    "    mlm = False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    train_dataset=data,\n",
    "    args=training_arg,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ETo_l9h2KPw_"
   },
   "outputs": [],
   "source": [
    "# Save the Quantized and Fine-tunned Model\n",
    "trainer.model.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "84gmjmXxRm7d",
    "outputId": "9b2d3c00-184f-41a5-8062-10ef6f4066d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] What is a large language model? [/INST] A large language model is a machine learning model that is trained on a large dataset of text, such as a large corpus of books or a large dataset of text from the internet. The model is designed to learn the patterns and structures of language, and to be able to generate text that is similar to the training data.\n",
      "\n",
      "Large language models are typically trained using a technique called deep learning, which involves using multiple layers of artificial neural networks to learn the patterns and structures of language. These models are often used for a variety of natural language processing tasks, such as language translation, text summarization, and text generation.\n",
      "\n",
      "Some examples of large language models include:\n",
      "\n",
      "* BERT (Bidirectional Encoder Representations from Transformers): A popular large language model developed by Google that has been trained on a large corpus of text from the internet.\n",
      "* RoBERTa\n"
     ]
    }
   ],
   "source": [
    "# Ignore warnings\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "# Run text generation pipeline with our new model\n",
    "prompt = \"What is a large language model?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYaY8oT_RtF5"
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del pipe\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "1554de8cc1f048ceb9bda876dfcccda7",
      "8267f02c5e7140f887940c953951ac6e",
      "acd116daf31243b7894e8b51a44f67e5",
      "256fcbb9bef74bfda99651a8ad842146",
      "8f6354552c3f4e6e970a87411bdb8385",
      "7f10078b4b0a44f898e9a736f784faa4",
      "9ee0516433d7498087e1d5e1017314e3",
      "5fad7d8c18114b289de9f0a4e72a07fd",
      "a9a9762807f74e20873cbb697d3e8945",
      "526ba12242054a8f956ed3061f5f1386",
      "8e509ee67ec54995a9b8aa3613082e82"
     ]
    },
    "id": "7Yeg_Q_9R0Zd",
    "outputId": "bcf93afa-da6e-4cdc-b7ab-510a84109a78"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1554de8cc1f048ceb9bda876dfcccda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reload model in FP16 and merge it with LoRA weights\n",
    "# we need to merge the fine tunned weights in the base model to create an final Quantized and Fine tunned model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map={\"\": 0},\n",
    ")\n",
    "model = PeftModel.from_pretrained(base_model, new_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Reload tokenizer to save it\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vy2YNnNdR4VP",
    "outputId": "064bc4f1-9472-4b1e-b104-550fa6cba282"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "\n",
      "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Enter your token (input will not be visible): \n",
      "Add token as git credential? (Y/n) n\n",
      "Token is valid (permission: fineGrained).\n",
      "The token `DoChat` has been saved to /root/.cache/huggingface/stored_tokens\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful.\n",
      "The current active token is: `DoChat`\n"
     ]
    }
   ],
   "source": [
    "# Login into hugging face to Push the model\n",
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296,
     "referenced_widgets": [
      "7104a6ee0b8945449ff480e2baae3e2e",
      "8ab17a5192004b9cbdde1d9b5d811206",
      "30beb4d3268c417b91758f30b15378f0",
      "240e6dde42594887991d0f25244e8231",
      "11d3eb0650704f9d9008187858caeeff",
      "291c5f09f0104397b650d16ee5c1c7f1",
      "27a97fd7db67446daf9f342ef8d147b9",
      "bb24cce9335a48df9504e53fd402de77",
      "7377473050d24211aecae5cbeeddb09e",
      "7d03e5a8d30141ddb1447cc0ddf7819a",
      "a0245411cf5c4ecc8b311391c8b6529b",
      "ee6b39e652234f89849e743b921170e2",
      "cac6f90b861b4730ad8f6790e45a1c64",
      "c17726c304e64bc88a77828f7261d5bf",
      "08b5be86b87c42918636d1813096b7cf",
      "34cbd9f12ed441218eb3005372a6583b",
      "a61e04c3ba824450aa72f9e39e1cf8f8",
      "c4c9d3e96f984b8f85c974d75ddbdb53",
      "b441583dd33241089727ef37508d5b3b",
      "db1e50a7b30745bf8d0a787b640523b4",
      "0217d13960a247a1baff968a1e23e32d",
      "d7624bc7638d44659146558f6a4f7068",
      "ebee72b717774b29be01be82754e69de",
      "3e1b99b2805a4c9ba1d0cfe433352241",
      "039d27f8beeb4340954adc2a61b59871",
      "1a4e74ba39cc4b6896089c24f09d7af1",
      "6367a06394cf43ca999cc801c9149d58",
      "1bf974f847164c2f8f045ed15029659f",
      "98b9d7bde1ad4037aa4175b47180a462",
      "92c7cc6429f8439aad8c5b2b57989ee5",
      "592851f4fe6646d49268ac1bbef8a65d",
      "7ea3ce4643a946baad0338ee260ecbd9",
      "c45b395d95a1489284b205638e540a2d",
      "fa5eec9cf813483c8586216535dcea2b",
      "d19a0fda753140fead781a2cbc597c9d",
      "b1dca64753324d4fa4918d4d3861b996",
      "b16be7654cb547c19280c8a6dd067750",
      "d35db015343049e3a2b10ef0b64f55a9",
      "bc442eb0fb1a41f4a4ce13a1cd5a105b",
      "53500bc05cea4035997ff54744ce36fd",
      "d69215cc21dc44e9acfa29a1926f536e",
      "5c4e91ac832f4fd784f9b9717fec2135",
      "8186e4ab601f4adab6f33c702b814c24",
      "8df03adf5ee147c59fda5bfcaba8b450",
      "5bedcc9c9dfe4b209933a53f22891e14",
      "1718e323d62f4b09a6db84f75da1d895",
      "9d52c687ccea45f49c4216d241aecabb",
      "1a8bd16cb614476abbfd2ab8b690bed9",
      "8b92177587ec49a1bd79c8eefa12faa2",
      "4241dcc57e07414ea11b36037ebd7e34",
      "e4e1b12dbf22494b8723f254e0ce23e3",
      "ad130fa18c534332bc2b8a36383624de",
      "8b2d3d711abf43869edc3f89cfd2c741",
      "157516f003ec4875a2de4cd2777e9244",
      "74011245da974b3ba98c9245795c1dcb",
      "589cb03f781f410c9e0da7c4e343047e",
      "fa03896fdc5e4b75a060d8461f77353f",
      "482dd79d42d04b8da1c0f49792ebe9a1",
      "12a0424e19ea45d3b2b8a0b5f69565c5",
      "34087ed100a342b2becd036e217888d7",
      "a90ee2c0af024a3685cd241f9c7345f7",
      "21c15521bc814e738a3d933b249b0886",
      "d5d3b8482381488aa2a3a3f45c632934",
      "a3441aaac49d40a4b9be72923fa7c046",
      "61e258c3bc0741b6a54782a72a1038af",
      "08591664a3bc4cabafd30ff85ccbcf67"
     ]
    },
    "id": "1CokRQA4SZUx",
    "outputId": "0a483cdc-0e37-4fe7-c484-8b39b2a98fac"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7104a6ee0b8945449ff480e2baae3e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee6b39e652234f89849e743b921170e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebee72b717774b29be01be82754e69de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa5eec9cf813483c8586216535dcea2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bedcc9c9dfe4b209933a53f22891e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589cb03f781f410c9e0da7c4e343047e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/Tarun9216/Llama-2-7b-DoChat-finetune/commit/45a1991ac2fe9ae35b8e247ea021fb91c03458f0', commit_message='Upload tokenizer', commit_description='', oid='45a1991ac2fe9ae35b8e247ea021fb91c03458f0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Tarun9216/Llama-2-7b-DoChat-finetune', endpoint='https://huggingface.co', repo_type='model', repo_id='Tarun9216/Llama-2-7b-DoChat-finetune'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Push the final model and the tokenizer to the Hugging face.\n",
    "model.push_to_hub(\"Tarun9216/Llama-2-7b-DoChat-finetune\", check_pr=True)\n",
    "\n",
    "tokenizer.push_to_hub(\"Tarun9216/Llama-2-7b-DoChat-finetune\",check_pr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xq5tXrtsZV5n"
   },
   "outputs": [],
   "source": [
    "# Now the Final Finetuned model is uploded to the hugging face and you can use this model like any other LLaMA Model."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
